---
title: "Prediction Modeling"
author: 'Hun Lee'
date: "12/4/2021"
output: github_document
---

# Data Preparation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r defaults, echo = FALSE, message = FALSE, warning = FALSE}
# load necessary packages
library(tidyverse)
library(spatstat)
library(glmnet)
library(ggpubr)
library(GGally)
library(pvclust)
library(cluster)
library(factoextra)
library(tidymodels)
library(vip)
library(rsample)
library(themis)
library(parsnip)
library(workflows)
library(dials)

# set knitr defaults
knitr::opts_chunk$set(
    echo      = TRUE
  , message   = FALSE
  , fig.width = 6
  , fig.asp   = .6
  , out.width = "90%"
)
# set theme defaults
theme_set(
  theme_bw() +
  theme(
    legend.position = "bottom"
    , plot.title    = element_text(hjust = 0.5)
    , plot.subtitle = element_text(hjust = 0.5)    
    , plot.caption  = element_text(hjust = 0.0)
  )
)
# set color scale defaults
options(
    ggplot2.continuous.colour = "viridis"
  , ggplot2.continuous.fill   = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete   = scale_fill_viridis_d
```

```{r unzip, echo = FALSE, message = FALSE, warning = FALSE}
jimzip <- function(csv_file, path) {
  # create full path to csv file
  full_csv <- paste0(path, "/", csv_file)
  # append ".zip" to csv file
  zip_file <- paste0(full_csv, ".zip")
  # unzip file
  unzip(zip_file)
  # read csv
  data_extract <- read_csv(csv_file)
  # be sure to remove file once unzipped (it will live in working directory)
  on.exit(file.remove(csv_file))
  # output data
  data_extract
}
census_data <- jimzip("census_filtered.csv", "./data")
```

```{r merge, echo = FALSE, message = FALSE, warning = FALSE}
health_data <-
  read_csv("./data/outcome_puma.csv") %>%
  rename(puma = puma10)
merged_data <- merge(census_data, health_data, by = "puma")
rm(census_data)
```

```{r clean, echo = FALSE, message = FALSE, warning = FALSE}
# Clean the merged census and outcomes data
cleaned_data = 
  merged_data %>% 
  # Remove variables less useful for analysis, including ones with high correlation with remaining variables
  select(-multyear, -ancestr1, -ancestr2, -labforce, -occ, -ind, -incwage, -occscore, -pwpuma00, -ftotinc, -hcovpub) %>% 
  # Remove duplicate rows
  distinct() %>% 
  # Rename variables
  rename(
    borough = countyfip,
    has_broadband = cihispeed,
    birthplace = bpl,
    education = educd,
    employment = empstat,
    personal_income = inctot,
    work_transport = tranwork,
    household_income = hhincome,
    on_foodstamps = foodstmp,
    family_size = famsize,
    num_children = nchild,
    US_citizen = citizen,
    puma_vacc_rate = puma_vacc_per,
    on_welfare = incwelfr,
    poverty_threshold = poverty
  ) %>% 
  # Recode variables according to data dictionary
  mutate(
    # Researched mapping for county
    borough = recode(
      borough,
      "5" = "Bronx",
      "47" = "Brooklyn",
      "61" = "Manhattan",
      "81" = "Queens",
      "85" = "Staten Island"
    ),
    rent = ifelse(
      rent == 9999, 0,
      rent
    ),
    household_income = ifelse(
      household_income %in% c(9999998,9999999), NA,
      household_income
    ),
    on_foodstamps = recode(
      on_foodstamps,
      "1" = "No",
      "2" = "Yes"
    ),
    has_broadband = case_when(
      has_broadband == "20" ~ "No",
      has_broadband != "20" ~ "Yes"
    ),
    sex = recode(
      sex,
      "1" = "Male",
      "2" = "Female"
    ),
    # Collapse Hispanic observation into race observation
    race = case_when(
      race == "1" ~ "White",
      race == "2" ~ "Black",
      race == "3" ~ "American Indian",
      race %in% c(4,5,6) ~ "Asian and Pacific Islander",
      race == 7 & hispan %in% c(1,2,3,4) ~ "Hispanic",
      race == 7 & hispan %in% c(0,9) ~ "Other",
      race %in% c(8,9) ~ "2+ races"
    ),
    birthplace = case_when(
      birthplace %in% 1:120 ~"US",
      birthplace %in% 121:950 ~ "Non-US",
      birthplace == 999 ~"Unknown"
    ),
    US_citizen = case_when(
      US_citizen %in% c(1,2) ~ "Yes",
      US_citizen %in% 3:8 ~"No",
      US_citizen %in% c(0,9) ~ "Unknown"
    ),
    # Chose languages based on highest frequency observed
    language = case_when(
      language == "1" ~ "English",
      language == "12" ~ "Spanish",
      language == "43" ~ "Chinese",
      language == "0" ~ "Unknown",
      language == "31" ~ "Hindi",
      !language %in% c(1,12,43,0,31) ~ "Other"
    ),
    # Collapse multiple health insurance variables into single variable
    health_insurance = case_when(
      hcovany == 1 ~ "None",
      hcovany == 2 & hcovpriv == 2 ~ "Private",
      hcovany == 2 & hcovpriv == 1 ~ "Public"
    ),
    education = case_when(
      education %in% 2:61 ~ "Less Than HS Graduate",
      education %in% 62:64 ~ "HS Graduate",
      education %in% 65:100 ~ "Some College",
      education %in% 110:113 ~ "Some College",
      education == 101 ~ "Bachelor's Degree",
      education %in% 114:116 ~ "Post-Graduate Degree",
      education %in% c(0,1,999) ~ "Unknown"
    ),
    employment = case_when(
      employment %in% c(0,3) ~ "Not in labor force",
      employment == 1 ~ "Employed",
      employment == 2 ~ "Unemployed"
    ),
    personal_income = ifelse(
      personal_income %in% c(9999998,9999999), NA,
      personal_income
    ),
    household_income = ifelse(
      household_income %in% c(9999998,9999999), NA,
      household_income
    ),
    on_welfare = case_when(
      on_welfare > 0 ~ "Yes",
      on_welfare == 0 ~ "No"
    ), 
    poverty_threshold = case_when(
      poverty_threshold >= 100 ~ "Above",
      poverty_threshold < 100 ~ "Below"
    ),
    work_transport = case_when(
      work_transport %in% c(31:37, 39) ~ "Public Transit",
      work_transport %in% c(10:20, 38) ~ "Private Vehicle",
      work_transport == 50 ~ "Bicycle",
      work_transport == 60 ~ "Walking",
      work_transport == 80 ~ "Worked From Home",
      work_transport %in% c(0, 70) ~ "Other"
    )
  ) %>% 
  # Convert hospitalization and death rates to pure percentages to match vax rate
  mutate(
    puma_hosp_rate = puma_hosp_rate / 1000,
    puma_death_rate = puma_death_rate / 1000
  ) %>% 
  # Eliminate columns no longer needed after transformation
  select(-hispan, -hcovany, -hcovpriv) %>% 
  # Relocate new columns
  relocate(health_insurance, .before = personal_income) %>% 
  relocate(poverty_threshold, .before = work_transport) %>% 
  relocate(on_welfare, .before = poverty_threshold) %>% 
  relocate(perwt, .before = cluster) %>% 
  # Create factor variables where applicable
  mutate(across(.cols = c(puma, borough, on_foodstamps, has_broadband, sex, race, birthplace, US_citizen, language, health_insurance, education, employment, on_welfare, poverty_threshold, work_transport), as.factor)) %>% 
  # Change levels of certain key factors for later analysis
  mutate(
    health_insurance = factor(health_insurance,
                              levels = c("None", "Public", "Private")),
    education = factor(education,
                       levels = c("Less Than HS Graduate", "HS Graduate", "Some College", "Bachelor's Degree", "Post-Graduate Degree", "Unknown"))
  )
```

```{r puma summary, echo = FALSE, message = FALSE, warning = FALSE}
# Example data frame with weightings for summary stats over each PUMA
nyc_puma_summary = cleaned_data %>% 
  # Note: do we need to filter to one individual per household for household weightings?
  group_by(puma) %>%
  summarize(
    total_people = sum(perwt),
    median_household_income = weighted.median(household_income, hhwt, na.rm = TRUE),
    perc_foodstamps = sum(hhwt[on_foodstamps == "Yes"]) * 100 / sum(hhwt),
    perc_broadband = sum(hhwt[has_broadband == "Yes"]) * 100 / sum(hhwt),
    perc_male = sum(perwt[sex == "Male"]) * 100 / sum(perwt),
    median_age = weighted.median(age, perwt, na.rm = TRUE),
    perc_white = sum(perwt[race == "White"]) * 100 / sum(perwt),
    perc_foreign_born = sum(perwt[birthplace == "Non-US"]) * 100 / sum(perwt),
    perc_citizen = sum(perwt[US_citizen == "Yes"]) * 100 / sum(perwt),
    perc_english = sum(perwt[language == "English"]) * 100 / sum(perwt),
    perc_college = sum(perwt[education %in% c("Some College", "Bachelor's Degree", "Post-Graduate Degree")]) * 100 / sum(perwt),
    perc_unemployed = sum(perwt[employment == "Unemployed"]) * 100 / sum(perwt),
    perc_insured = sum(perwt[health_insurance %in% c("Private", "Public")]) * 100 / sum(perwt),
    median_personal_income = weighted.median(personal_income, perwt, na.rm = TRUE),
    perc_welfare = sum(perwt[on_welfare == "Yes"]) * 100 / sum(perwt),
    perc_poverty = sum(perwt[poverty_threshold == "Below"]) * 100 / sum(perwt),
    perc_public_transit = sum(perwt[work_transport == "Public Transit"]) * 100 / sum(perwt),
    covid_hosp_rate = median(puma_hosp_rate),
    covid_vax_rate = median(puma_vacc_rate),
    covid_death_rate = median(puma_death_rate)
  )
```

```{r}
# Example data frame with weightings for summary stats over each PUMA
nyc_puma_summary = cleaned_data %>% 
  # Note: do we need to filter to one individual per household for household weightings?
  group_by(puma) %>%
  summarize(
    total_people = sum(perwt),
    median_household_income = weighted.median(household_income, hhwt, na.rm = TRUE),
    perc_foodstamps = sum(hhwt[on_foodstamps == "Yes"]) * 100 / sum(hhwt),
    perc_broadband = sum(hhwt[has_broadband == "Yes"]) * 100 / sum(hhwt),
    perc_male = sum(perwt[sex == "Male"]) * 100 / sum(perwt),
    median_age = weighted.median(age, perwt, na.rm = TRUE),
    perc_white = sum(perwt[race == "White"]) * 100 / sum(perwt),
    perc_foreign_born = sum(perwt[birthplace == "Non-US"]) * 100 / sum(perwt),
    perc_citizen = sum(perwt[US_citizen == "Yes"]) * 100 / sum(perwt),
    perc_english = sum(perwt[language == "English"]) * 100 / sum(perwt),
    perc_college = sum(perwt[education %in% c("Some College", "Bachelor's Degree", "Post-Graduate Degree")]) * 100 / sum(perwt),
    perc_unemployed = sum(perwt[employment == "Unemployed"]) * 100 / sum(perwt),
    perc_insured = sum(perwt[health_insurance %in% c("Private", "Public")]) * 100 / sum(perwt),
    median_personal_income = weighted.median(personal_income, perwt, na.rm = TRUE),
    perc_welfare = sum(perwt[on_welfare == "Yes"]) * 100 / sum(perwt),
    perc_poverty = sum(perwt[poverty_threshold == "Below"]) * 100 / sum(perwt),
    perc_public_transit = sum(perwt[work_transport == "Public Transit"]) * 100 / sum(perwt),
    covid_hosp_rate = median(puma_hosp_rate),
    covid_vax_rate = median(puma_vacc_rate),
    covid_death_rate = median(puma_death_rate)
  )
```

## Prediction Modeling

### Risk scoring

We want to develop a method to score PUMAs on risk of not achieving herd immunity from vaccination. Let's say that herd immunity occurs at 70% vaccination rate, for our purposes.

```{r}
# Define binary outcome and set of predictors
# 1 indicates BELOW 70% vaccination rate
logistic_df = nyc_puma_summary %>% 
  mutate(
    below_herd_vax = as.factor(ifelse(covid_vax_rate >= 70, 0, 1))
  ) %>% 
  select(-puma, -total_people, -covid_hosp_rate, -covid_death_rate, -covid_vax_rate)

# Define predictors matrix
x = model.matrix(below_herd_vax ~ ., logistic_df)[,-1]

# Define outcomes
y = logistic_df$below_herd_vax
```


```{r}
# Set seed for reproducibility
set.seed(1234)

# Split into training and testing data sets
# 41 rows in training
# 14 rows in testing
vax_split <- initial_split(logistic_df, strata = below_herd_vax)
vax_train <- training(vax_split)
vax_test <- testing(vax_split)

# Set another seed for reproducibility
set.seed(123)

# k-fold cross validation step repeated 30 times using stratification
vax_folds <- vfold_cv(vax_train, strata = below_herd_vax, v = 10, repeats = 1)
```


```{r}
# Center and scale predictors
# Down-sample based on outcome
vax_rec <- recipe(below_herd_vax ~ ., data = vax_train) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  step_downsample(below_herd_vax)
  
```


```{r}
# mixture = 1 & glmnet are for lasso regression
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_mode("classification") %>%
  set_engine("glmnet")

wf <- workflow(vax_rec, lasso_spec) 

# Search penalty over grid
vax_grid <- grid_regular(penalty(range = c(-5,0)), levels = 20) 
```

```{r}
doParallel::registerDoParallel()
set.seed(2021)
# Compute a set of performance metrics for set of tuning parameters
vax_rs <-
  tune::tune_grid(
    wf,
    resamples = vax_folds,
    grid = vax_grid
  )
```


```{r}
# Plot accuracy and ROC/AUC
autoplot(vax_rs)
```

```{r}
# Identify best ROC/AUC
final_penalty <-
  vax_rs %>%
  select_best(metric = "roc_auc")

# Perform final model fit using optimal ROC/AUC
final_rs <-
  wf %>% 
  finalize_workflow(final_penalty) %>%
  last_fit(vax_split)

# Determine performance metrics on final binary classifier: ~79% accuracy
collect_metrics(final_rs)

# Collect predictive performance metrics across splits
collect_predictions(final_rs)

# Plot predictive performance measures across splits
collect_predictions(final_rs) %>%
  roc_curve(below_herd_vax, .pred_0) %>%
  autoplot()
``` 


```{r}
# Extract most important variables for binary classification in final lasso model
wf %>% 
  finalize_workflow(final_penalty) %>%
  fit(vax_train) %>%
  extract_fit_parsnip() %>%
  vi() %>%
  mutate(Variable = fct_reorder(Variable, Importance)) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() + 
  scale_fill_manual(values = c("orange", "skyblue"))
```

## Cross Validaiton Using Caret

```{r}
library(caret)
library(mlbench)
```

## Repeated Cross Validation
```{r}
set.seed(777)
vax_cv <- trainControl(method = "repeatedcv", number = 5, repeats = 100, 
                       savePredictions = T
                       )

# Goal is to find optimal lambda
lasso_model <- train(below_herd_vax ~ ., data = logistic_df,
                     method = "glmnet",
                     trControl = vax_cv,
                     tuneGrid = expand.grid(
                       .alpha = 1,
                       .lambda = seq(0.0001, 1, length = 100)),
                     family = "binomial")

lasso_model
```

## Result from training data
```{r}
coef <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda)

sub_lasso <-
  subset(lasso_model$pred, lasso_model$pred$lambda == lasso_model$bestTune$lambda)

caret::confusionMatrix(table(sub_lasso$pred, sub_lasso$obs))

```



```{r}

#lambda <- 10^seq(-2, 3, length = 0.1)

lambda  <- seq(0.0001, 1, length = 100)

lambda_opt = lasso_model$bestTune$lambda


broom::tidy(lasso_model$finalModel) %>% 
select(term, lambda, estimate) %>% 
complete(term, lambda, fill = list(estimate = 0) ) %>% 
filter(term != "(Intercept)") %>% 
ggplot(aes(x = log(lambda, 10), y = estimate, group = term, color = term)) + 
geom_path() + 
geom_vline(xintercept = log(lambda_opt, 10), color = "blue", size = 1.2) +
theme(legend.position = "none")


```


## Getting Risk Prediction for each puma
```{r, message = FALSE, warning = FALSE}
lambda <- lasso_model$bestTune$lambda
lasso_fit = glmnet(x, y, lambda = lambda, family = "binomial")
risk_predictions = (round((predict(lasso_fit, x, type = "response"))*100, 1))


puma <- nyc_puma_summary %>% 
  select(puma)

vax <- logistic_df %>% 
  select(below_herd_vax)



risk_prediction <- 
  bind_cols(puma, vax, as.vector(risk_predictions)) %>%
  rename(risk_prediciton = ...3)

risk_prediction
```


```{r, message = FALSE, warning = FALSE}
library(nycgeo)
library(sf)
library(leaflet)
library(htmlwidgets)
library(shiny)
library(stringi)

nyc_hh_summary <- read_csv("./data_for_regression.csv")

risk_prediction_map_data <-
  nyc_boundaries(geography = "puma") %>%
  mutate(puma = puma_id) %>%
  left_join(risk_prediction, by = "puma") 
```
```{r}
map_dataset <-
  risk_prediction_map_data %>%
  merge(nyc_hh_summary, by = "puma") %>%
  mutate(median_household_income = round(median_household_income, digits = 0),
         perc_insured = round(perc_insured, digits = 1),
         perc_unemployed = round(perc_unemployed, digits = 1),
         perc_poverty = round(perc_poverty, digits = 1)
         ) %>% 
  mutate(median_household_income = 
           formatC(median_household_income,format = "d",big.mark = ",")) %>%
  mutate(covid_vacc_rate = round(covid_vacc_rate, digits = 1),
         covid_death_rate_2020 = round(covid_death_rate_2020, digits = 0),
         covid_death_rate_2021 = round(covid_death_rate_2021, digits = 0),
         covid_hosp_rate_2020 = round(covid_hosp_rate_2020, digits = 0),
         covid_hosp_rate_2021 = round(covid_hosp_rate_2021, digits = 0)
         ) 
```

```{r}
map_dataset_label <-
  map_dataset %>% 
  select(puma, perc_asian, perc_white, perc_black, perc_hispanic, perc_other) %>%
  pivot_longer(perc_asian:perc_other, values_to = "value", names_to = "race") %>%
  arrange(puma, desc(value)) %>% 
  mutate(race = recode(race, 
                       "perc_asian" = "Asian/Pacific Islander:",
                       "perc_white" = "White:",
                       "perc_black" = "Black:",
                       "perc_hispanic" = "Hispanic:",
                       "perc_other" = "Other:",
                       )) %>%
  mutate(value = round(value, digits = 1)) %>%
  mutate(label = paste(race, value)) %>%
  mutate(label = paste0(label, "%")) %>%
  select(puma, label)

first_race <-
  map_dataset_label %>% 
  group_by(puma) %>% 
  filter(row_number() == 1) %>%
  mutate(label1 = label) %>%
  select(puma, label1)

second_race <-
  map_dataset_label %>% 
  group_by(puma) %>% 
  filter(row_number() == 2) %>%
  mutate(label2 = label) %>%
  select(puma, label2)

third_race <-
map_dataset_label %>% 
  group_by(puma) %>% 
  filter(row_number() == 3) %>%
  mutate(label3 = label) %>%
  select(puma, label3)

fourth_race <-
map_dataset_label %>% 
  group_by(puma) %>% 
  filter(row_number() == 4) %>%
  mutate(label4 = label) %>%
  select(puma, label4)

fifth_race <-
map_dataset_label %>% 
  group_by(puma) %>% 
  filter(row_number() == 5) %>%
  mutate(label5 = label) %>%
  select(puma, label5)

map_dataset_label <-
  bind_cols(first_race, second_race, third_race, fourth_race, fifth_race)
```


```{r}
labels <- sprintf("<strong>%s<strong> <br/> %s <br/> %s <br/> %s <br/> %s <br/> %s <br/>
                  Insurance Rate: %s%% <br/> Median Household Income: $%s <br/> Vaccination
                  Risk Score: %g%%", map_dataset$puma_name, map_dataset_label$label1,
                  map_dataset_label$label2, map_dataset_label$label3,
                  map_dataset_label$label4, map_dataset_label$label5,
                  map_dataset$perc_insured, map_dataset$median_household_income,
                  map_dataset$risk_prediciton) %>% lapply(htmltools::HTML)

pal <- colorBin(palette = "OrRd", 9, domain = map_dataset$risk_prediciton)

map_interactive <- map_dataset %>% st_transform(crs = "+init=epsg:4326") %>%
  leaflet() %>%
  addProviderTiles(provider = "CartoDB.Positron") %>%
  addPolygons(label = labels,
              labelOptions = labelOptions(
    style = list("font-weight" = "normal", 
                 padding = "1px 2px"),
    textsize = "11px",  sticky = TRUE,
    opacity = 0.55
    ),
              stroke = FALSE,
              opacity = 0.01,
              smoothFactor = .5,
              fillOpacity = 0.7,
              fillColor = ~pal(risk_prediciton),
              highlightOptions = highlightOptions(weight = 5, 
                                                  fillOpacity = 1,
                                                  color = "black",
                                                  opacity = 1,
                                                  bringToFront = TRUE
                                                  )) %>%
  addLegend("bottomright",
            pal = pal,
            values = ~risk_prediciton,
            title = "Vaccination Percentage Risk Prediction",
            opacity = 0.7
            )

map_interactive
```

